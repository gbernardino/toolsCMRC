{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas, numpy as np\n",
    "import parsingDatabaseUtils, re, os\n",
    "import xml, itertools, collections, xml.etree.ElementTree as ET\n",
    "import tqdm, importlib, dateparser, dateparser.search\n",
    "import parseMeasurementsByDay\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathDataIds = 'Venezolanas2'\n",
    "#procedimientosDesc = pandas.read_csv(os.path.join(pathDataIds, 'procedimientosID.csv'), index_col = 0)\n",
    "#diagnosticos = pandas.read_csv(os.path.join(pathDataIds, 'diagnosis.csv'), index_col = 0)\n",
    "#medicamentos = pandas.read_csv(os.path.join(pathDataIds, 'medicamentos.csv'), index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathFolder = 'Casos2019'\n",
    "casos = pandas.read_csv(pathFolder + '/casos.csv', index_col = 0)\n",
    "pacientes = pandas.read_csv(pathFolder + '/pacientes.csv', index_col = 0)\n",
    "pacientes.index = pacientes.index.map(str)\n",
    "\n",
    "registros = pandas.read_csv(pathFolder + '/registros.csv', index_col = 0)\n",
    "registros.index = registros.index.map(str)\n",
    "\n",
    "#diagnosis = pandas.read_csv('Venezolanas2/diagnosis.csv', index_col = 0)\n",
    "procedimientos = pandas.read_csv(pathFolder + '/procedimientos.csv', index_col = 0)\n",
    "procedimientos.index = procedimientos.index.map(str)\n",
    "registrosByCaso = registros.groupby('Caso')\n",
    "\n",
    "entriesInfirmery = pandas.read_csv(pathFolder + '/enfermeriaMedidas.csv', index_col = 0)\n",
    "entriesInfirmeryByCase = entriesInfirmery.groupby('IdAdmision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsingData.procedures import classificationProcedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/notFoundInAutomatic.pkl', 'rb') as file:\n",
    "    errors = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a case\n",
    "Identify all  cases with an associated procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all  cases with an associated procedure\n",
    "import difflib\n",
    "def sameField(et1, et2, field, equal = lambda i, j: i == j):\n",
    "    return equal(str(et1.find(field).text), str(et2.find(field).text))\n",
    "\n",
    "def similarTexts(s1, s2, th = .80):\n",
    "    d =difflib.SequenceMatcher(a = s1, b = s2)\n",
    "    r = sum((i.size for i in d.get_matching_blocks())) / min(len(s1), len(s2))\n",
    "    return r > th\n",
    "\n",
    "def areSameIntervention(r1, r2):\n",
    "    \"\"\"\n",
    "    Decides if they are the same based on the type\n",
    "    \"\"\"\n",
    "    r1ET = ET.fromstring(r1.XmlDescripcion)\n",
    "    r2ET = ET.fromstring(r2.XmlDescripcion)\n",
    "\n",
    "    #Date is the same / similar\n",
    "    isDateSame = sameField(r1ET, r2ET, './/cabecera//fechaCirugia')\n",
    "    # Type of intervention is the same\n",
    "    isInterventionSame = sameField(r1ET, r2ET, './/idProcedimiento')\n",
    "\n",
    "    # Description\n",
    "    isDescriptionSame = sameField(r1ET, r2ET, './/descripcion', equal = similarTexts)\n",
    "    field = './/descripcion'\n",
    "    return isInterventionSame and (isDateSame or  isDescriptionSame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interventionToCase = {}\n",
    "caseToProcedureBirths = {}\n",
    "checkSeveralProcedures = []\n",
    "unknownProc = []\n",
    "procsNotFound = []\n",
    "for i,r  in  tqdm.tqdm_notebook(registros.loc[registros.CodigoRegistro == 145].iterrows(), total = len(registros)):\n",
    "    if r.Caso not in casos.index:\n",
    "        continue\n",
    "        \n",
    "    if isinstance(r.RegistroXML, str) and '<row NombreCampo=\"IdDescripcion\"' in r.RegistroXML:\n",
    "        et = ET.fromstring(r.RegistroXML)\n",
    "        idDescripcionProcedimiento = et.find('.//row[@NombreCampo=\"IdDescripcion\"]').get('ValorCampo')\n",
    "        if idDescripcionProcedimiento not in procedimientos.index:\n",
    "            print('Procedimiento asociado con caso %s no se encuentra (# %s)' % (r.Caso, idDescripcionProcedimiento))\n",
    "            continue\n",
    "        interventionToCase[idDescripcionProcedimiento] = (r.Caso, r.NumeroHistoria, i)\n",
    "        procedureType = re.findall('<idProcedimiento>([a-zA-Z0-9]*)</idProcedimiento>', procedimientos.loc[idDescripcionProcedimiento].XmlDescripcion)[0]\n",
    "        try:\n",
    "            typeProc = classificationProcedures[procedureType]\n",
    "        except:\n",
    "            typeProc = 'unknown'\n",
    "            unknownProc.append(procedureType)\n",
    "        if typeProc == 'p':\n",
    "            \n",
    "            #If it is the case that they need to stop labour because a c-section is needed\n",
    "            #if '<postDiagnosticoPrincipal>O821 - PARTO POR CESAREA DE EMERGENCIA</postDiagnosticoPrincipal>' in \\\n",
    "            #    procedimientos.loc[idDescripcionProcedimiento].XmlDescripcion:\n",
    "            #    print('continue')\n",
    "            #    continue\n",
    "                \n",
    "            if r.Caso in caseToProcedureBirths:\n",
    "                r1 = caseToProcedureBirths[r.Caso]\n",
    "                r2 =procedimientos.loc[idDescripcionProcedimiento]\n",
    "                if areSameIntervention(r1, r2):\n",
    "                    if r1.FechaDescripcion < r2.FechaDescripcion:\n",
    "                        caseToProcedureBirths[r.Caso] = r2\n",
    "                else:\n",
    "                    checkSeveralProcedures.append((r.Caso, caseToProcedureBirths[r.Caso],procedimientos.loc[idDescripcionProcedimiento] ))\n",
    "                    procedureType2 = re.findall('<idProcedimiento>([a-zA-Z0-9]*)</idProcedimiento>', caseToProcedureBirths[r.Caso].XmlDescripcion)[0]\n",
    "\n",
    "                    print('error, varios procedimientos en caso: ', r.Caso, procedureType, procedureType2)\n",
    "                    if r1.FechaRegistro > r2.FechaRegistro:\n",
    "                        caseToProcedureBirths[r.Caso] = procedimientos.loc[idDescripcionProcedimiento]\n",
    "            else:\n",
    "                caseToProcedureBirths[r.Caso] = procedimientos.loc[idDescripcionProcedimiento]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "birthCases = casos.loc[caseToProcedureBirths.keys()]\n",
    "birthCases['dayOperation'] = birthCases.FechaHora.map(lambda s: datetime.datetime.strptime(s.split()[0], '%Y-%m-%d') if s == s else s)\n",
    "birthCasesByDay = birthCases.groupby('dayOperation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for c in caseToProcedureBirths:\n",
    "    if c in casos.index and str(casos.loc[c].Paciente) in pacientes.index:\n",
    "        res[c] =  pacientes.loc[str(casos.loc[c].Paciente)].Identificacion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cF = 0\n",
    "cNF  = 0\n",
    "incorrectId = []\n",
    "for id, i, date in errors:\n",
    "    i = i.strip()\n",
    "    if id[-1] != '0':\n",
    "        continue\n",
    "    if i  in res.values() or 'VEN' + i  in res.values() or 'V' + i  in res.values():\n",
    "        cF += 1\n",
    "    elif i.replace('V', 'VEN') in res.values()  or i.replace('VEN', \"V\") in res.values():\n",
    "        cF += 1\n",
    "    elif i not in pacientes.index:\n",
    "        if date in birthCasesByDay.groups:\n",
    "            incorrectId.append((i, date))\n",
    "    else:\n",
    "        print(i)\n",
    "        cNF += 1\n",
    "print(cF, cNF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = 0\n",
    "incor = 0\n",
    "for i, date in incorrectId:\n",
    "    for p in pacientes.loc[map(lambda i: str(int(i)),birthCasesByDay.get_group(date).Paciente)].Identificacion:\n",
    "        if similarTexts(i, p, th= .75):\n",
    "            cor += 1\n",
    "            print(i, p)\n",
    "            break\n",
    "    else:\n",
    "        incor += 1\n",
    "print(cor, incor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, r1, r2 in checkSeveralProcedures:\n",
    "    if areSameIntervention(r1, r2):\n",
    "        print (c, 'same')\n",
    "    else:\n",
    "        print(c, 'Different')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import birthDatasetStructure\n",
    "importlib.reload(birthDatasetStructure)\n",
    "# Split in newborn\n",
    "registrosByCaso = registros.groupby('Caso')\n",
    "registrosByPatient = registros.groupby('NumeroHistoria')\n",
    "\n",
    "twins = {}\n",
    "processedDatasets = {}\n",
    "countCaseNotFound = 0\n",
    "countPatientNotFound = 0\n",
    "\n",
    "for c, p in tqdm.tqdm_notebook(caseToProcedureBirths.items()):\n",
    "    if c not in casos.index:\n",
    "        print('ERROR! Caso no encontrado', c)\n",
    "        countCaseNotFound += 1\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    if p.IdPaciente not in registrosByCaso.get_group(c).NumeroHistoria.values:\n",
    "        print('ERROR! Pacientes inconsistente en caso', c, p.IdPaciente)\n",
    "        countPatientNotFound += 1\n",
    "        continue\n",
    "\n",
    "    if str(p.IdPaciente) not in pacientes.index:\n",
    "        print('ERROR! Pacientes no encontrado', p.IdPaciente)\n",
    "        countPatientNotFound += 1\n",
    "        break\n",
    "        continue\n",
    "     \n",
    "    if c in entriesInfirmeryByCase.groups:\n",
    "        entriesInf = entriesInfirmeryByCase.get_group(c)\n",
    "    else:\n",
    "        entriesInf = pandas.DataFrame()\n",
    "    \n",
    "    data = birthDatasetStructure.BirthDataset(c, casos.loc[c], p, registrosByCaso.get_group(c), \n",
    "                                                              pacientes,entriesInf)\n",
    "    if data.epicrisis is None:\n",
    "        continue\n",
    "    for i, r in registrosByPatient.get_group(data.motherData.name if 'v' in data.motherData.name .lower() else int(data.motherData.name)).iterrows():\n",
    "        inRange = lambda d1, d2: (datetime.datetime.strptime(d1, '%Y-%m-%d %H:%M:%S.%f') - datetime.datetime.strptime(d2, '%Y-%m-%d %H:%M:%S.%f')) < datetime.timedelta(days = 7)\n",
    "        if r.CodigoRegistro == 20:\n",
    "            if r.FechaAsignacionRegistro < data.epicrisis.FechaAsignacionRegistro and inRange(data.epicrisis.FechaAsignacionRegistro, r.FechaAsignacionRegistro):\n",
    "                data.registersMother.append(r)\n",
    "    #Discard twins:\n",
    "    #if len(data.registrosRecienNacido) > 1:\n",
    "    #    twins[c] = data\n",
    "    #    continue\n",
    "    processedDatasets[c] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look for twins\n",
    "wrongRegisters = []\n",
    "twins = {}\n",
    "print('Errors ! More than one register found')\n",
    "for c, p in processedDatasets.items():\n",
    "    if any((r  in getAllDiagnosis(p.epicrisis) for r in ['Z372', 'O300'])) :\n",
    "        #Twins  / \n",
    "        #print(len(p.registrosRecienNacido), getAllDiagnosis(p.epicrisis))\n",
    "        twins[c] = p\n",
    "        continue\n",
    "    elif len(p.registrosRecienNacido) > 1 and  any((r  in getAllDiagnosis(p.epicrisis) for r in ['Z370'])) :\n",
    "        wrongRegisters.append(c)\n",
    "        print(c, getAllDiagnosis(p.epicrisis),  len(p.registrosRecienNacido))\n",
    "        continue\n",
    "    elif len(p.registrosRecienNacido) > 1:\n",
    "        for k in p.registrosRecienNacido:\n",
    "            if 'GEMEL' in parsingDatabaseUtils.findInXML('TexTarea_MotivoConsulta', p.registrosRecienNacido[k][k].RegistroXML):\n",
    "                twins[c] = p\n",
    "                break\n",
    "        else:\n",
    "            print(c, getAllDiagnosis(p.epicrisis),  len(p.registrosRecienNacido))        \n",
    "            wrongRegisters.append(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c,data in twins.items():\n",
    "    if len(data.registrosRecienNacido) != 2:\n",
    "        continue\n",
    "    for i, v  in enumerate(data.splitTwins()):\n",
    "        processedDatasets[c + '-%d' % i] = v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in wrongRegisters:\n",
    "    del processedDatasets[c]\n",
    "for c in twins:\n",
    "    del processedDatasets[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caseTransformer\n",
    "importlib.reload(caseTransformer)\n",
    "importlib.reload(caseTransformer.parseNewbornRegister)\n",
    "importlib.reload(caseTransformer.parseEpicrisis)\n",
    "importlib.reload(caseTransformer.parseEchos)\n",
    "importlib.reload(caseTransformer.parsingDatabaseUtils)\n",
    "importlib.reload(caseTransformer.parseMeasurementsByDay)\n",
    "importlib.reload(caseTransformer.parseInterventions)\n",
    "importlib.reload(caseTransformer.parseDiagnosis)\n",
    "importlib.reload(caseTransformer.parseMedicaments)\n",
    "importlib.reload(caseTransformer.parseMeasurementsByDay.parsingDatabaseUtils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resSIP = {}\n",
    "echoLines= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process\n",
    "\n",
    "for c, d in tqdm.notebook.tqdm(processedDatasets.items()):\n",
    "    #if c in resSIP or c != 'AD324070':\n",
    "    #    continue\n",
    "    cT = caseTransformer.CaseTransformer(data = d)    \n",
    "    resSIP[c],  echoLines[c]= cT.processAll()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "errorsAvoidable = []\n",
    "for c in processedDatasets:\n",
    "    if echoLines[c] or  resSIP[c].get('VAR_0057','') not in ['07/06/1954', '']:\n",
    "        continue\n",
    "    if 'incorrect_echo' in resSIP[c]:\n",
    "        errorsAvoidable.append(c)\n",
    "    errors.append(c)\n",
    "print(len(errors), len(errorsAvoidable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egsByEcho = {}\n",
    "for c in processedDatasets:\n",
    "    dBirth = parsingDatabaseUtils.parseDate(resSIP[c].get('VAR_0284'),'datetime')\n",
    "    EGDays = 0\n",
    "    if resSIP[c].get('VAR_0057','') not in ['07/06/1954', '']:\n",
    "        dateFUM = parsingDatabaseUtils.parseDate(resSIP[c].get('VAR_0057'), 'datetime')\n",
    "        EGDays = (dBirth - dateFUM).days\n",
    "    elif echoLines[c]:\n",
    "        egs = []\n",
    "        for d, eg in echoLines[c].items():\n",
    "            try:\n",
    "                d = parsingDatabaseUtils.parseDate(d, 'datetime')\n",
    "                EG = (dBirth - d).days + float(eg) *7 \n",
    "                egs.append(EG)\n",
    "            except:\n",
    "                pass\n",
    "        egsByEcho[c] = egs\n",
    "        EGDays = np.median(egs)        \n",
    "        \n",
    "    if EGDays:     \n",
    "        resSIP[c]['VAR_0198'] = EGDays//7\n",
    "        resSIP[c]['VAR_0199'] = EGDays%7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for c,v in resSIP.items():\n",
    "    if 'VAR_0198' not in v:\n",
    "        errors.append(c)\n",
    "print(len(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echoLinesFilt = list(filter(lambda s: s, echoLines.values()))\n",
    "len(echoLinesFilt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dead = ['AD332737', 'AD328745', 'AD329407', 'AD333575', 'AD337298',\n",
    "       'AD342790', 'AD344552', 'AD325267', 'AD347333', 'AD348308',\n",
    "       'AD347707', 'AD347283', 'AD347201', 'AD347068', 'AD335797',\n",
    "       'AD334130', 'AD331887', 'AD348926', 'AD349064', 'AD349527',\n",
    "       'AD349551', 'AD349784', 'AD351643', 'AD350591', 'AD350887',\n",
    "       'AD350768', 'AD354417', 'AD356504', 'AD355652', 'AD359734']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caso = dead[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnborn = registros.loc[registros.CodigoRegistro == 165]\n",
    "l1, l2, l3 = [], [], []\n",
    "for _, r in  rnborn.iterrows():\n",
    "    et = ET.fromstring(r.RegistroXML)\n",
    "    d1 = parsingDatabaseUtils.findInXML('InputText_Calsificacion', et)\n",
    "    d2 = parsingDatabaseUtils.findInXML('TexTarea_AtencionNeonatalReanimacion', et)\n",
    "    d3 = parsingDatabaseUtils.findInXML('InputText_Presentacion', et)\n",
    "    l1.append(d1)\n",
    "    l2.append(d2)\n",
    "    l3.append(d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resDF = pandas.DataFrame.from_dict(resSIP, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(resDF['incorrect_echo'] == resDF['incorrect_echo'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resDF.columns.map(lambda c: 'VAR' in str(c) or c in ['FECHA', 'HORA', 'VERSION', 'USUARIO', 'ID01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resDF2 = resDF[resDF.columns[resDF.columns.map(lambda c: 'VAR' in str(c) or c in ['FECHA', 'HORA', 'VERSION', 'USUARIO', 'ID01'])]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varsDone = {}\n",
    "dfNamesVars = pandas.read_excel('parsingData/variables_sip.xlsx')\n",
    "renameDict = {k : (k + ' = ' + v) for  k, v in zip(dfNamesVars['Unnamed: 2'].values, dfNamesVars['Unnamed: 3'].values)}\n",
    "renameDict['VAR_0189'] = 'Corticoides / semana de inicio'\n",
    "\n",
    "for p in resDF2.columns:\n",
    "    varsDone[renameDict.get(p,p)] = np.mean(resDF2[p] == resDF2[p])\n",
    "    print(renameDict.get(p,p), np.mean(resDF2[p] == resDF2[p]))\n",
    "df =pandas.DataFrame.from_dict(varsDone, orient = 'index', columns = ['Fraccion presente en tabla'])\n",
    "df.to_excel('estadisticasRecuperacion.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedDatasets['AD365346'].epicrisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resDF.to_csv('Data/results1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "\n",
    "From here on, it is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allMedicaments = {}\n",
    "allInterventions = {}\n",
    "for c, p in tqdm.tqdm_notebook(processedDatasets.items()):\n",
    "    m, i = getMedicamentsInterventionsFromEpicrisis(p.epicrisis)\n",
    "    \n",
    "    for _, v in m.items():\n",
    "        for c, desc, _ in v:\n",
    "            allMedicaments[c] = desc\n",
    "            \n",
    "    for _, v in i.items():\n",
    "        for c, desc, _ in v:\n",
    "            allInterventions[c] = desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in allInterventions.items():\n",
    "    if k not in parseInterventions.actionIntervention:\n",
    "        print('actionIntervention[\"%s\"] = \"\" #%s' % (k, parsingDatabaseUtils.remove_diacritics(v)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('parseMedicaments.py', 'w') as f:\n",
    "    f.write('actionMedicament = {} \\n')\n",
    "    for k, v in allMedicaments.items():\n",
    "        f.write('actionMedicament[\"%s\"] = \"\" #%s\\n' % (k, parsingDatabaseUtils.remove_diacritics(v)))\n",
    "        \n",
    "with open('parseInterventions.py', 'w') as f:\n",
    "    f.write('actionIntervention = {} \\n')\n",
    "    for k, v in allInterventions.items():\n",
    "        f.write('actionIntervention[\"%s\"] = \"\" #%s\\n' % (k, parsingDatabaseUtils.remove_diacritics(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseMedicamentsInterventionsByDay(parsingDatabaseUtils.findInXML('MedicamentosAdministrado', et))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ET.fromstring(p.epicrisis.RegistroXML)\n",
    "parsingDatabaseUtils.findInXML('MedicamentosAdministrado', et)\n",
    "print(parsingDatabaseUtils.findInXML('ProcedimientosAsociados', et))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parseDiagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDiagnosis = set()\n",
    "for c, p in tqdm.tqdm_notebook(processedDatasets.items()):\n",
    "    allDiagnosis.update(set(getAllDiagnosis(p.epicrisis)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDiagnosis = set(list(allDiagnosis) + list(parseDiagnosis.actionsDiagnosis.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in sorted(allDiagnosis):\n",
    "    #if a in actionsDiagnosis:\n",
    "    #    pass\n",
    "    if a not in diagnosticos.index:\n",
    "        continue \n",
    "    if False and a in parseDiagnosis.actionsDiagnosis:\n",
    "        print(\"actionsDiagnosis['%s'] = '%s' #%s\"% (a,parseDiagnosis.actionsDiagnosis[a], diagnosticos.loc[a, 'dfg']))\n",
    "    else: \n",
    "        print(\"actionsDiagnosis['%s'] = '' #%s\"% (a,diagnosticos.loc[a, 'dfg']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsingDatabaseUtils.prettyPrintXML(p.epicrisis.RegistroXML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[p.epicrisis.Diagnostico, p.epicrisis.CodigoDiagnosticoRelacionado1,  p.epicrisis.CodigoDiagnosticoRelacionado2, p.epicrisis.CodigoDiagnosticoRelacionado3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c,  _ in df[df['sufrimientoFetal'] == 'SI'].iterrows():\n",
    "    et = ET.fromstring(processedDatasets[c].epicrisis.RegistroXML)\n",
    "    print(parsingDatabaseUtils.findInXML('MedicamentosAdministrado', et ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noEpicrsis = []\n",
    "noIngreso = []\n",
    "\n",
    "noDischarge = []\n",
    "for c, p in processedDatasets.items():\n",
    "    if p.epicrisis is None:\n",
    "        noDischarge.append(c)\n",
    "    if p.ingreso is None:\n",
    "        noIngreso.append(c)\n",
    "print('no emergency' , len(noIngreso), 'no epi', len(noDischarge), len(processedDatasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pacientes.loc[str(casos.loc[noIngreso[0], 'Paciente'])], noIngreso[0], casos.loc[noIngreso[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count = 0\n",
    "#for c, p in processedDatasets.items():\n",
    "#    if p.epicrisis is None:\n",
    "#        continue\n",
    "#    et = ET.fromstring(p.epicrisis.RegistroXML)\n",
    "#    ant = parsingDatabaseUtils.fullCleanTxt(parsingDatabaseUtils.findInXML('AntecedentesHTML', et))\n",
    "#    if not 'paracli' in ant:\n",
    "#        count += 1\n",
    "#    else:\n",
    "#        print(ant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(dateparser.parse('6/1/20') - dateparser.parse('4/11/19')).days // 7, (dateparser.parse('6/1/20') - dateparser.parse('4/11/19')).days %7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateDifferenceDays(d1, d2):\n",
    "    p1 = dateparser.parse(d1) if isinstance(d1, str) else d1\n",
    "    p2 = dateparser.parse(d2) if isinstance(d2, str) else d2\n",
    "    return (p1 - p2).days\n",
    "\n",
    "def egByEcho(date, res):\n",
    "    bestDiff = 365\n",
    "    bestEg  = 0\n",
    "    date =dateparser.parse(date)\n",
    "    for i in range(10):\n",
    "        if 'echo_%d_eg' in res:\n",
    "            eg = res['echo_%d_eg' % i] \n",
    "            dateEcho = dateparser.parse(res['echo_%d_date' % i])\n",
    "            \n",
    "            if np.abs(dateDifferenceDays(date, dateEcho)) < bestDiff:\n",
    "                bestDiff = np.abs(dateDifferenceDays(date, dateEcho))\n",
    "                bestEg = float(eg) * 7 + dateDifferenceDays(date, dateEcho)\n",
    "    return bestEg\n",
    " \n",
    "def dateToGestationDays(date, res):\n",
    "    \"\"\"\n",
    "    Checks wether the event is before or after labour.\n",
    "    \n",
    "    - If it is before, return the number of gestation days\n",
    "    - Otherwise, the time before.\n",
    "    \"\"\"\n",
    "    #If FUM\n",
    "\n",
    "    #Otherwise, use the closest echo\n",
    "    else:\n",
    "        days = egByEcho(date, res)\n",
    "    return days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 'AD284225'\n",
    "k = next(iter(processedDatasets[c].registrosRecienNacido.keys()))\n",
    "parsingDatabaseUtils.prettyPrintXML(processedDatasets[c].registrosRecienNacido[k][k].RegistroXML)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(processedDatasets.keys())[884]\n",
    "count = 0\n",
    "for c in processedDatasets.keys():\n",
    "    try:\n",
    "        if 'ECLAMPS' in processedDatasets[c].epicrisis.RegistroXML:\n",
    "            count += 1\n",
    "            #parsingDatabaseUtils.prettyPrintXML(processedDatasets[c].epicrisis.RegistroXML)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(processedDatasets.keys())[244]\n",
    "parsingDatabaseUtils.prettyPrintXML(processedDatasets[c].epicrisis.RegistroXML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraClinics = {}\n",
    "for c, p in tqdm.tqdm_notebook(processedDatasets.items()):\n",
    "    if p.epicrisis is None:\n",
    "        continue\n",
    "    d =parseParaclinics(p.epicrisis)\n",
    "    paraClinics[c] = paraclinicsToDF(parseParaclinics(p.epicrisis))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfParaClinics.noParaclinicalTestsConfirmed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfParaClinics = pandas.DataFrame.from_dict(paraClinics, orient = 'index')\n",
    "dfParaClinics = dfParaClinics[sorted(dfParaClinics.columns)]\n",
    "dfParaClinics.to_csv('paraclinics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p, d in processedDatasets.items():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMedicamentsAndpProcedures(d.epicrisis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "pars =HTMLParser()\n",
    "\n",
    "def splitByDate(txt):\n",
    "    txt = pars.unescape(txt)\n",
    "    txtByDate = txt.split('Fecha:')\n",
    "    txtByDate = map(lambda s: s.strip(), txtByDate)\n",
    "    txtByDate = {m.split()[0] : m for m in txtByDate if m}\n",
    "            print(m)\n",
    "\n",
    "    processedByDate = {}\n",
    "    for v,m in txtByDate.items():\n",
    "        processedByDate[v] =  re.findall('-([0-9a-zA-Z\\- ]+) - (.+?)- Cantidad: ([0-9]+)', m, re.MULTILINE)\n",
    "    return processedByDate\n",
    "def getMedicamentsAndpProcedures(epicrisis):\n",
    "    et = ET.fromstring(epicrisis.RegistroXML)\n",
    "    medicamentos = parsingDatabaseUtils.findInXML('MedicamentosAdministrado' , et)\n",
    "    procedimientos = parsingDatabaseUtils.findInXML('ProcedimientosAsociados' , et)\n",
    "    #return medicamentos, procedimientos\n",
    "    return splitByDate(medicamentos), splitByDate(procedimientos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsingDatabaseUtils.prettyPrintXML(d.epicrisis.RegistroXML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, p in paraClinics.items():\n",
    "    if len(p) != 1:\n",
    "        print(paraClinics[c]['text'])\n",
    "        print('-----------------')\n",
    "        for k in p:\n",
    "            if k != 'text':\n",
    "                print(k, p[k])\n",
    "        print('-----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge2DF(df1, df2):\n",
    "    \"\"\"\n",
    "    Returns the merge of 2 df with the same index (1 to 1)\n",
    "    In case of columns with the same name, check if the values are the same (or one of them is NA)\n",
    "    Creates new error column to see where there has been problems\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(dateparser.parse('4/4/17'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = processedDatasets[list(processedDatasets.keys())[672]].epicrisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antecedentes = parsingDatabaseUtils.findInXML('AntecedentesHTML', epi.RegistroXML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(parsingDatabaseUtils)\n",
    "parsingDatabaseUtils.parseEchographies(antecedentes, cleanText=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(parsingDatabaseUtils.date, '6 de marzo de 1995', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(parsingDatabaseUtils)\n",
    "pp = parsingDatabaseUtils.prettyPrintXML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registros.loc[registros.Caso == 'AD348509']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
